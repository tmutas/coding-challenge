# Data Engineering Coding Challenge
## Introduction
The task of the challenge is to ingest a CSV file of Kafka events, transform it and load it into a data warehouse.

The approach is basically an ETL process, so 
- Extracting the raw data and making it columnar
- Transforming the data, removing unwanted columns, and pulling out additional columns
- Loading the data into the data warehouse, by modeling dimensions and facts.  

Please bear in mind that I don't have a greate overview yet of all the technologies and tools out there, in order to decide which approach would be the optimal one for this ETL task.

## Getting Started
The project is written in Python, as it's versatile in the variety of tools it can interact with and it's the language I'm most proficient in. 

#### Dependencies
I use poetry as a dependency management tool for Python. In order to use it, you first need to run `pip install poetry` in your chosen Python enviroment, and afterwards just launch `poetry install` with an optional `--without dev` flag, if you don't intend to explore the data through pandas in a jupyter notebook or similar or don't need python developement tools.

- `pyspark`
Note that you need to have at least a JDK installed. Spark does work locally without a Hadoop cluster

#### Running it
The pipeline is wrapped into a simple `main.py` script that runs the pipeline, and takes the file path as input, as well as where the Spark/Hive data warehouse shall be written to.
For production purposes, there is obviously better choices to orchestrate the pipeline, which I didn't implement due to time constraints.
For example, Apache Airflow could be used to run the provided functions as separate Operators. 

## Data Extraction
I decided to use PySpark for the ETL task, as it can also handle large datasets in a distributed fashion. There is also the Structured Streaming extension, which could be an option in a real-life setting, where the Kafka data can be consumed directly with (hopefully) minor code changes to the rest of the ETL pipeline.

The extraction step uses the standard PySpark reader to load the multiline CSV structure, which in turn contains data serialized as a JSON string.
In the subsequent step, PySpark json functions are used to parse the strings, and load and combine the data into a flattened DataFrame. This provides a good basis for further transformations.

## Data Transformation
The transformation step is also performed in PySpark. 

Of course, this step can be as well performed in a variety of other ways than on a Spark DataFrame.
For example, by loading it into some database before, and using SQL afterwards.
I assume dbt is a suitable choice on top of that.

#### Removing empty columns
In the provided data sample, there were a couple of fields which were entirely filled with null data. 
Due to lack of domain knowledge about the data source, I decided to disregard them completely, as I don't know more about how to model a sensible dimension out of it, and reduce the complexity.
In a real-life setting, one might consider implementing a more automated approach on loading any dimensions, if those columns can expect useful data at some point.

#### Parsing message column
The message column also contained some semi-structured data, which I chose to extract using regexes.
It also contained messages generated by a test system, which was a sign for me to remove it. 
I decided to do it explicitely on finding a "test message" string, but depending on what's expected in the message, it could also be on filtering the rows where the extracted field is null. 

However, for an informed decision, more domain knowledge about the data generating system is required.
For example, different types of messages might be expected. Then there could be additional fields extracted, with others being allowed to be null, or the data can be split by a TBD "message type", and modeled with separate dimensions for the different message types.

## Data Loading
Modeling the Data Warehouse dimensions always depends on the use case at hand. With this in mind, I had to model it with the limited knowledge I could get from the data sample only. In a real-life scenario, this would of course take into account knowledge about the source system and possible analysis use cases.

#### Choosing dimensions
To provide an example, I created dimensions for some of the columns. 
There is better tools or libraries to manage this modeling, but I just wanted to keep simple by using plain SQL queries.
Some dimensions can be shared across multiple columns, for example the IP addresses.

I decided so skip modeling a separate dimension for some pure integer columns to keep the scope limited.

#### Handling bad data
I made sure that empty or null values do not go in the creation of the dimension, but instead should be mapped to the default dimension key.
However, domain knowledge can help developing better suited strategies for handling bad data. There is open questions as to what counts as faulty value and whether it should be dropped or kept.

#### Dimension keys 
For the most part, I just use row number as a dimension identifier, but again depending on the expected data, there can be different and better strategies for creating surrogate keys. 

#### Code structure
I tried to make the code in the pipeline to be adaptable for different choices of DBMSs. Therefore there is an abstract BaseQueries class that specifies which statement are to be run (and e.g. enables type checking and code completion)
This class should then be implemented for a DBMS, in this case Spark/Hive. 
The class can then also handle connection setup, etc.

